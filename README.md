# AI Workforce Lab

> **By Appy Hour Labs** Â· Public experiment in responsible AI deployment Â· Powered by **EvalPal**

---

What does it actually look like to deploy AI agents as persistent team members â€” with real governance, real failure modes, and a public paper trail of what happened?

That's what this repository answers. In real time. With the receipts committed.

---

## What We're Building

**EvalPal** is the product: tooling and methodology for evaluating AI agent quality in production workflows. The AI Workforce Lab is EvalPal's first test environment â€” we're building the eval harness by being the thing it evaluates.

**The experiment:** deploy AI agents into operational roles (documentation, sales research, content, financial analysis, technical strategy) using a tiered autonomy model. Document everything. Ship the governance alongside the features.

**The constraint:** safety first. Always. If the choice is between moving fast and being safe, we choose safe and write a task about it.

---

## The Autonomy Model

| Phase | What it means |
|---|---|
| **Phase A** (current) | All outbound actions require human approval. Eval gates run before content approaches external recipients. Agents draft, analyze, propose â€” they do not send or publish autonomously. |
| **Phase B** (future) | Agents may publish within approved scope without per-action human review. Requires measurable evidence from Phase A. Criteria: [`POLICIES/phase-a-to-b.md`](POLICIES/phase-a-to-b.md) |

---

## Repository Map

```
ai-workforce-lab/
â”œâ”€â”€ AGENTS/          â† role definitions for each AI and human operator
â”œâ”€â”€ DOCS/SHOW/       â† public weekly episodes (the documentary)
â”‚   â””â”€â”€ episodes/
â”œâ”€â”€ EVALS/           â† quality gate rubrics and results
â”œâ”€â”€ POLICIES/        â† governance, safety, OAuth, posting, escalation rules
â”œâ”€â”€ PROJECTS/        â† strategic project definitions
â”œâ”€â”€ RUNBOOKS/        â† operational how-tos: CI, session handoff, incident response
â””â”€â”€ TASKS/           â† atomic execution tasks (30â€“60 min each)
```

---

## Key Policies

| Policy | What it covers |
|---|---|
| [AI Safety Charter](POLICIES/ai-safety-charter.md) | Non-negotiable principles |
| [OAuth Policy](POLICIES/oauth-policy.md) | Scope restrictions, no domain-wide delegation |
| [Posting Policy](POLICIES/posting-policy.md) | Who can publish, eval gate requirements |
| [Escalation Policy](POLICIES/escalation-policy.md) | Severity levels, when to ping the founder |
| [Phase A â†’ B Criteria](POLICIES/phase-a-to-b.md) | Measurable promotion gates |

---

## The Team

| Account | Type | Role |
|---|---|---|
| `matt@appyhourlabs.com` | ğŸ‘¤ Human | Founder â€” final authority on all policy and autonomy decisions |
| `ai@appyhourlabs.com` | ğŸ¤– AI Ops | General operations, technical and research tasks |
| `doc@appyhourlabs.com` | ğŸ¤– AI Ops | Documentary agent â€” writes the weekly show |
| `sales@appyhourlabs.com` | ğŸ¤– AI Ops | Outreach research and drafting â€” Phase A gated |
| `media@appyhourlabs.com` | ğŸ¤– AI Ops | Content and distribution â€” Phase A gated |
| `legal@appyhourlabs.com` | ğŸ‘¤ Human only | Legal review â€” no AI delegation |
| `security@appyhourlabs.com` | ğŸ‘¤ Human only | Credential and security management â€” human only |
| `billing@appyhourlabs.com` | ğŸ‘¤ Human only | Financial accounts â€” human only, no exceptions |

Full role specs: [`AGENTS/`](AGENTS/)

---

## The Show

Every week, `doc@appyhourlabs.com` files an episode documenting what shipped, what broke, what was decided, and what we learned. Public, honest, occasionally funny.

- [Episode 000 â€” Origin Log](DOCS/SHOW/episodes/000-origin-log.md)
- [Episode 001 â€” The AI Office Moves In](DOCS/SHOW/episodes/001-ai-office-moves-in.md)
- [Episode template](<DOCS/SHOW/episodes/_TEMPLATE.md>)

---

## Status

*Last updated: 2026-02-21*

| Project | Status | What it is |
|---|---|---|
| [0001 â€” Google Workspace Hardening](PROJECTS/0001-google-workspace-hardening.md) | ğŸ”µ Active | MFA, API access controls, OAuth scope audit for all AI ops accounts |
| [0002 â€” Mac Mini AI Office Setup](PROJECTS/0002-mac-mini-ai-office-setup.md) | ğŸ”µ Active | Dedicated hardened machine for running agent pipelines |

Autonomy tier: **Phase A** â€” all outbound requires human approval.

---

## Start Here

New here? Read in this order:

1. [Episode 000 â€” Origin Log](DOCS/SHOW/episodes/000-origin-log.md) â€” why this project exists
2. [Episode 001 â€” The AI Office Moves In](DOCS/SHOW/episodes/001-ai-office-moves-in.md) â€” what we did first and why
3. [AI Safety Charter](POLICIES/ai-safety-charter.md) â€” the non-negotiables
4. [`AGENTS.md`](AGENTS.md) â€” who does what (human and AI)

---

## Safety

This project operates under a formal safety charter. The short version:

- No agent acts autonomously on external communications in Phase A
- No secrets, credentials, or PII in this repository â€” ever
- All agent scope changes require human sign-off
- Every incident gets logged; nothing gets quietly swept

Full details: [`POLICIES/ai-safety-charter.md`](POLICIES/ai-safety-charter.md)

---

## How to Follow

- **Watch this repo** â€” GitHub â†’ Watch â†’ "All Activity" for commits, PRs, and issues
- **Star it** â€” helps others find the experiment
- **Read the episodes** â€” [`DOCS/SHOW/episodes/`](DOCS/SHOW/episodes/) is updated weekly by `doc@appyhourlabs.com`
- **Open an issue** â€” questions, observations, or suggested tasks are welcome using the [task template](.github/ISSUE_TEMPLATE/task.md)

No mailing list, no newsletter, no algorithm. Just commits.

---

## Contributing

1. Read [`POLICIES/ai-safety-charter.md`](POLICIES/ai-safety-charter.md) first
2. Select a task from [`TASKS/`](TASKS/) or a project from [`PROJECTS/`](PROJECTS/)
3. Small PRs. Every PR needs a **Security Considerations** section
4. No secrets, no PII, ISO dates only

See [`AGENTS.md`](AGENTS.md) for the full contributor context guide.

---

## License

[MIT](LICENSE) â€” use freely, cite honestly.
